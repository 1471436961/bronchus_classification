"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
åŒ…å«æ•°æ®åŠ è½½ã€æ•°æ®å¢å¼ºã€å†…å­˜ç®¡ç†ç­‰åŠŸèƒ½
"""

import os
import random
import shutil
import json
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from collections import Counter
import warnings

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageFilter
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import matplotlib.pyplot as plt
import seaborn as sns


class DatasetSplitter:
    """æ•°æ®é›†åˆ’åˆ†å™¨ï¼Œæ”¯æŒåˆ†å±‚é‡‡æ ·"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        
    def split_data_stratified(
        self, 
        org_path: str, 
        split_path: str, 
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        test_ratio: float = 0.15,
        min_samples_per_class: int = 10
    ) -> Dict[str, int]:
        """
        åˆ†å±‚é‡‡æ ·åˆ’åˆ†æ•°æ®é›†ï¼Œç¡®ä¿ç±»åˆ«å¹³è¡¡
        
        Args:
            org_path: åŸå§‹æ•°æ®è·¯å¾„
            split_path: è¾“å‡ºè·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹  
            test_ratio: éªŒè¯é›†æ¯”ä¾‹
            min_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å°‘æ ·æœ¬æ•°
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        print(f"{'='*20} å¼€å§‹åˆ†å±‚æ•°æ®é›†åˆ’åˆ† {'='*20}")
        
        # éªŒè¯æ¯”ä¾‹
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯
        all_samples = []
        class_counts = Counter()
        
        for class_name in os.listdir(org_path):
            class_path = os.path.join(org_path, class_name)
            if not os.path.isdir(class_path):
                continue
                
            images = [f for f in os.listdir(class_path) 
                     if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            
            if len(images) < min_samples_per_class:
                warnings.warn(f"ç±»åˆ« {class_name} åªæœ‰ {len(images)} ä¸ªæ ·æœ¬ï¼Œå°‘äºæœ€å°è¦æ±‚ {min_samples_per_class}")
                
            for img in images:
                all_samples.append((os.path.join(class_path, img), class_name))
                class_counts[class_name] += 1
        
        # æå–è·¯å¾„å’Œæ ‡ç­¾
        X = [sample[0] for sample in all_samples]
        y = [sample[1] for sample in all_samples]
        
        # ç¬¬ä¸€æ¬¡åˆ’åˆ†ï¼šåˆ†ç¦»å‡ºéªŒè¯é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_ratio, 
            stratify=y, random_state=self.random_state
        )
        
        # ç¬¬äºŒæ¬¡åˆ’åˆ†ï¼šä»å‰©ä½™æ•°æ®ä¸­åˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size_adjusted = val_ratio / (train_ratio + val_ratio)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted,
            stratify=y_temp, random_state=self.random_state
        )
        
        # åˆ›å»ºç›®å½•å¹¶å¤åˆ¶æ–‡ä»¶
        splits = {
            'train': (X_train, y_train),
            'val': (X_val, y_val), 
            'test': (X_test, y_test)
        }
        
        stats = {}
        for split_name, (X_split, y_split) in splits.items():
            split_dir = os.path.join(split_path, split_name)
            os.makedirs(split_dir, exist_ok=True)
            
            split_counts = Counter(y_split)
            stats[split_name] = dict(split_counts)
            
            for src_path, class_name in zip(X_split, y_split):
                dst_dir = os.path.join(split_dir, class_name)
                os.makedirs(dst_dir, exist_ok=True)
                
                dst_path = os.path.join(dst_dir, os.path.basename(src_path))
                shutil.copy2(src_path, dst_path)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self._save_split_stats(split_path, stats, class_counts)
        self._plot_split_distribution(split_path, stats)
        
        print(f"{'='*20} æ•°æ®é›†åˆ’åˆ†å®Œæˆ {'='*20}")
        return stats
    
    def _save_split_stats(self, split_path: str, stats: Dict, original_counts: Counter):
        """ä¿å­˜åˆ’åˆ†ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = os.path.join(split_path, 'split_statistics.json')
        
        full_stats = {
            'original_distribution': dict(original_counts),
            'split_distribution': stats,
            'total_samples': {
                'original': sum(original_counts.values()),
                'train': sum(stats['train'].values()),
                'val': sum(stats['val'].values()),
                'test': sum(stats['test'].values())
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(full_stats, f, indent=2, ensure_ascii=False)
            
        print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    
    def _plot_split_distribution(self, split_path: str, stats: Dict):
        """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾"""
        try:
            # å‡†å¤‡æ•°æ®
            classes = list(stats['train'].keys())
            train_counts = [stats['train'].get(cls, 0) for cls in classes]
            val_counts = [stats['val'].get(cls, 0) for cls in classes]
            test_counts = [stats['test'].get(cls, 0) for cls in classes]
            
            # åˆ›å»ºå›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            # å †å æŸ±çŠ¶å›¾
            x = np.arange(len(classes))
            width = 0.8
            
            ax1.bar(x, train_counts, width, label='Train', alpha=0.8)
            ax1.bar(x, val_counts, width, bottom=train_counts, label='Val', alpha=0.8)
            ax1.bar(x, test_counts, width, 
                   bottom=np.array(train_counts) + np.array(val_counts), 
                   label='Test', alpha=0.8)
            
            ax1.set_xlabel('Classes')
            ax1.set_ylabel('Sample Count')
            ax1.set_title('Dataset Split Distribution by Class')
            ax1.set_xticks(x)
            ax1.set_xticklabels(classes, rotation=45, ha='right')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # æ¯”ä¾‹é¥¼å›¾
            total_train = sum(train_counts)
            total_val = sum(val_counts)
            total_test = sum(test_counts)
            
            sizes = [total_train, total_val, total_test]
            labels = ['Train', 'Val', 'Test']
            colors = ['#ff9999', '#66b3ff', '#99ff99']
            
            ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            ax2.set_title('Overall Dataset Split Ratio')
            
            plt.tight_layout()
            plt.savefig(os.path.join(split_path, 'split_distribution.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {os.path.join(split_path, 'split_distribution.png')}")
            
        except Exception as e:
            print(f"ç»˜åˆ¶åˆ†å¸ƒå›¾æ—¶å‡ºé”™: {e}")


class MedicalImageAugmentation:
    """åŒ»å­¦å›¾åƒä¸“ç”¨æ•°æ®å¢å¼º"""
    
    @staticmethod
    def get_light_augmentation(input_size: Tuple[int, int]) -> A.Compose:
        """è½»åº¦æ•°æ®å¢å¼ºï¼Œé€‚åˆé«˜è´¨é‡æ•°æ®"""
        return A.Compose([
            A.Resize(input_size[0], input_size[1]),
            A.HorizontalFlip(p=0.5),
            A.RandomRotate90(p=0.3),
            A.ShiftScaleRotate(
                shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5
            ),
            A.RandomBrightnessContrast(
                brightness_limit=0.2, contrast_limit=0.2, p=0.5
            ),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    @staticmethod
    def get_medium_augmentation(input_size: Tuple[int, int]) -> A.Compose:
        """ä¸­ç­‰å¼ºåº¦æ•°æ®å¢å¼º"""
        return A.Compose([
            A.Resize(input_size[0], input_size[1]),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.3),
            A.RandomRotate90(p=0.3),
            A.ShiftScaleRotate(
                shift_limit=0.15, scale_limit=0.15, rotate_limit=25, p=0.6
            ),
            A.OneOf([
                A.ElasticTransform(p=0.3),
                A.GridDistortion(p=0.3),
                A.OpticalDistortion(p=0.3),
            ], p=0.3),
            A.RandomBrightnessContrast(
                brightness_limit=0.3, contrast_limit=0.3, p=0.6
            ),
            A.OneOf([
                A.GaussNoise(var_limit=(10, 50), p=0.3),
                A.GaussianBlur(blur_limit=(3, 7), p=0.3),
                A.MotionBlur(blur_limit=7, p=0.3),
            ], p=0.4),
            A.HueSaturationValue(
                hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.4
            ),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    @staticmethod
    def get_heavy_augmentation(input_size: Tuple[int, int]) -> A.Compose:
        """å¼ºæ•°æ®å¢å¼ºï¼Œé€‚åˆæ•°æ®ä¸è¶³çš„æƒ…å†µ"""
        return A.Compose([
            A.Resize(input_size[0], input_size[1]),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.4),
            A.RandomRotate90(p=0.4),
            A.Transpose(p=0.3),
            A.ShiftScaleRotate(
                shift_limit=0.2, scale_limit=0.2, rotate_limit=35, p=0.7
            ),
            A.OneOf([
                A.ElasticTransform(alpha=1, sigma=50, p=0.4),
                A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.4),
                A.OpticalDistortion(distort_limit=0.3, shift_limit=0.3, p=0.4),
            ], p=0.5),
            A.RandomBrightnessContrast(
                brightness_limit=0.4, contrast_limit=0.4, p=0.7
            ),
            A.OneOf([
                A.GaussNoise(var_limit=(10, 80), p=0.4),
                A.GaussianBlur(blur_limit=(3, 9), p=0.4),
                A.MotionBlur(blur_limit=9, p=0.4),
                A.MedianBlur(blur_limit=7, p=0.3),
            ], p=0.6),
            A.HueSaturationValue(
                hue_shift_limit=30, sat_shift_limit=40, val_shift_limit=30, p=0.5
            ),
            A.OneOf([
                A.CLAHE(clip_limit=2, p=0.3),
                A.Sharpen(p=0.3),
                A.Emboss(p=0.3),
            ], p=0.4),
            A.CoarseDropout(
                max_holes=8, max_height=32, max_width=32, 
                min_holes=1, min_height=8, min_width=8, p=0.3
            ),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    @staticmethod
    def get_test_transform(input_size: Tuple[int, int]) -> A.Compose:
        """æ¨ç†æ—¶çš„å˜æ¢ï¼ˆæ— å¢å¼ºï¼‰"""
        return A.Compose([
            A.Resize(input_size[0], input_size[1]),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])


class Dataset(Dataset):
    """é«˜æ•ˆçš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒç¼“å­˜å’Œå¤šç§å¢å¼ºç­–ç•¥"""
    
    def __init__(
        self,
        data_dir: str,
        transform: Optional[A.Compose] = None,
        class_to_idx: Optional[Dict[str, int]] = None,
        cache_images: bool = False,
        image_extensions: Tuple[str, ...] = ('.jpg', '.jpeg', '.png', '.bmp')
    ):
        self.data_dir = data_dir
        self.transform = transform
        self.cache_images = cache_images
        self.image_extensions = image_extensions
        
        # æ„å»ºæ ·æœ¬åˆ—è¡¨
        self.samples, self.class_to_idx = self._make_dataset(class_to_idx)
        self.classes = list(self.class_to_idx.keys())
        
        # å›¾åƒç¼“å­˜
        self._image_cache = {} if cache_images else None
        
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬, {len(self.classes)} ä¸ªç±»åˆ«")
        if cache_images:
            print("å¯ç”¨å›¾åƒç¼“å­˜æ¨¡å¼")
    
    def _make_dataset(self, class_to_idx: Optional[Dict[str, int]]) -> Tuple[List[Tuple[str, int]], Dict[str, int]]:
        """æ„å»ºæ•°æ®é›†æ ·æœ¬åˆ—è¡¨"""
        if class_to_idx is None:
            # è‡ªåŠ¨æ„å»ºç±»åˆ«ç´¢å¼•
            classes = sorted([d for d in os.listdir(self.data_dir) 
                            if os.path.isdir(os.path.join(self.data_dir, d))])
            class_to_idx = {cls: idx for idx, cls in enumerate(classes)}
        
        samples = []
        for class_name, class_idx in class_to_idx.items():
            class_dir = os.path.join(self.data_dir, class_name)
            if not os.path.isdir(class_dir):
                continue
                
            for filename in os.listdir(class_dir):
                if filename.lower().endswith(self.image_extensions):
                    path = os.path.join(class_dir, filename)
                    samples.append((path, class_idx))
        
        return samples, class_to_idx
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:
        path, target = self.samples[index]
        
        # ä»ç¼“å­˜æˆ–ç£ç›˜åŠ è½½å›¾åƒ
        if self._image_cache is not None and path in self._image_cache:
            image = self._image_cache[path]
        else:
            image = self._load_image(path)
            if self._image_cache is not None:
                self._image_cache[path] = image
        
        # åº”ç”¨å˜æ¢
        if self.transform is not None:
            if isinstance(image, np.ndarray):
                transformed = self.transform(image=image)
                image = transformed['image']
            else:
                # å¦‚æœæ˜¯PILå›¾åƒï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„
                image_np = np.array(image)
                transformed = self.transform(image=image_np)
                image = transformed['image']
        else:
            # é»˜è®¤è½¬æ¢ä¸ºtensor
            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
            else:
                image = transforms.ToTensor()(image)
        
        return image, target
    
    def _load_image(self, path: str) -> np.ndarray:
        """åŠ è½½å›¾åƒï¼Œè¿”å›RGBæ ¼å¼çš„numpyæ•°ç»„"""
        try:
            # ä½¿ç”¨OpenCVåŠ è½½ï¼ˆBGRæ ¼å¼ï¼‰
            image = cv2.imread(path)
            if image is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {path}")
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            return image
            
        except Exception as e:
            print(f"åŠ è½½å›¾åƒå¤±è´¥ {path}: {e}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºfallback
            return np.zeros((224, 224, 3), dtype=np.uint8)
    
    def get_class_weights(self) -> torch.Tensor:
        """è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        class_counts = Counter([target for _, target in self.samples])
        total_samples = len(self.samples)
        num_classes = len(self.class_to_idx)
        
        weights = torch.zeros(num_classes)
        for class_idx, count in class_counts.items():
            weights[class_idx] = total_samples / (num_classes * count)
        
        return weights


class CustomDataLoader:
    """é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨å·¥å‚"""
    
    @staticmethod
    def create_dataloader(
        dataset: Dataset,
        batch_size: int,
        shuffle: bool = True,
        num_workers: Optional[int] = None,
        pin_memory: bool = True,
        persistent_workers: bool = True,
        prefetch_factor: int = 2,
        use_weighted_sampler: bool = False
    ) -> DataLoader:
        """
        åˆ›å»ºé«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±
            num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            pin_memory: æ˜¯å¦ä½¿ç”¨å›ºå®šå†…å­˜
            persistent_workers: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
            prefetch_factor: é¢„å–å› å­
            use_weighted_sampler: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨
        """
        if num_workers is None:
            num_workers = min(8, os.cpu_count() or 1)
        
        # å¦‚æœä½¿ç”¨åŠ æƒé‡‡æ ·å™¨ï¼Œåˆ™ä¸èƒ½åŒæ—¶shuffle
        sampler = None
        if use_weighted_sampler and hasattr(dataset, 'get_class_weights'):
            class_weights = dataset.get_class_weights()
            sample_weights = [class_weights[target] for _, target in dataset.samples]
            sampler = WeightedRandomSampler(
                weights=sample_weights,
                num_samples=len(dataset),
                replacement=True
            )
            shuffle = False
        
        return DataLoader(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=pin_memory and torch.cuda.is_available(),
            persistent_workers=persistent_workers and num_workers > 0,
            prefetch_factor=prefetch_factor if num_workers > 0 else 2,
            drop_last=True if shuffle else False
        )


def calculate_dataset_statistics(data_dir: str, sample_size: int = 1000) -> Dict[str, float]:
    """è®¡ç®—æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰"""
    print(f"è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼Œé‡‡æ · {sample_size} å¼ å›¾åƒ...")
    
    # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„
    image_paths = []
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                image_paths.append(os.path.join(root, file))
    
    # éšæœºé‡‡æ ·
    if len(image_paths) > sample_size:
        image_paths = random.sample(image_paths, sample_size)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    pixel_values = []
    for path in image_paths:
        try:
            image = cv2.imread(path)
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pixel_values.append(image.reshape(-1, 3))
        except Exception as e:
            print(f"è·³è¿‡å›¾åƒ {path}: {e}")
    
    if not pixel_values:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒï¼Œä½¿ç”¨ImageNetç»Ÿè®¡å€¼")
        return {
            'mean': [0.485, 0.456, 0.406],
            'std': [0.229, 0.224, 0.225]
        }
    
    # åˆå¹¶æ‰€æœ‰åƒç´ å€¼
    all_pixels = np.vstack(pixel_values).astype(np.float32) / 255.0
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean = np.mean(all_pixels, axis=0).tolist()
    std = np.std(all_pixels, axis=0).tolist()
    
    stats = {'mean': mean, 'std': std}
    print(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯: mean={mean}, std={std}")
    
    return stats


# ä½¿ç”¨ç¤ºä¾‹å’Œæ¼”ç¤ºå‡½æ•°
def run_preprocessing():
    """æ¼”ç¤ºæ•°æ®é¢„å¤„ç†æµç¨‹"""
    print("ğŸš€ æ•°æ®é¢„å¤„ç†æ¼”ç¤º")
    
    # 1. æ•°æ®é›†åˆ’åˆ†
    splitter = DatasetSplitter(random_state=42)
    
    # 2. åˆ›å»ºå¢å¼ºç­–ç•¥
    input_size = (224, 224)
    
    train_transform = MedicalImageAugmentation.get_medium_augmentation(input_size)
    val_transform = MedicalImageAugmentation.get_test_transform(input_size)
    test_transform = MedicalImageAugmentation.get_test_transform(input_size)
    
    print("âœ… æ•°æ®å¢å¼ºç­–ç•¥åˆ›å»ºå®Œæˆ")
    
    # 3. åˆ›å»ºæ•°æ®é›†ï¼ˆç¤ºä¾‹ï¼‰
    # train_dataset = Dataset(
    #     data_dir="data/data_split/train",
    #     transform=train_transform,
    #     cache_images=False  # æ ¹æ®å†…å­˜æƒ…å†µå†³å®š
    # )
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # train_loader = DataLoader.create_dataloader(
    #     dataset=train_dataset,
    #     batch_size=16,
    #     shuffle=True,
    #     use_weighted_sampler=True  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    # )
    
    print("âœ… é¢„å¤„ç†æµç¨‹æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    run_preprocessing()