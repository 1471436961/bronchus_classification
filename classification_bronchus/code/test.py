#!/usr/bin/env python3
"""
é«˜æ•ˆçš„è¯„ä¼°è„šæœ¬ - æ”¯æ°”ç®¡åˆ†ç±»é¡¹ç›®
ä½¿ç”¨é«˜æ•ˆçš„é…ç½®å’Œæ•°æ®å¤„ç†ç³»ç»Ÿ
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import List, Optional, Dict, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve

from config import Config, ConfigTemplates
from data_process import Dataset, CustomDataLoader, MedicalImageAugmentation

# é˜²æ­¢OMPé”™è¯¯
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class SimpleEvaluator:
    """é«˜æ•ˆçš„æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config: Config, save_dir: str = "./evaluation_results"):
        self.config = config
        self.device = torch.device(config.system.device)
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
    def load_model(self, model_path: str) -> nn.Module:
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.config.model.create_model()
        
        # åŠ è½½æƒé‡
        if model_path.endswith('.pth'):
            if 'checkpoint' in model_path or 'best_model' in model_path:
                # å®Œæ•´æ£€æŸ¥ç‚¹
                checkpoint = torch.load(model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
            else:
                # åªæœ‰æ¨¡å‹æƒé‡
                model.load_state_dict(torch.load(model_path, map_location=self.device))
        
        model = model.to(self.device)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return model
    
    def create_test_loader(self) -> Tuple[DataLoader, List[str]]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        print(f"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # åˆ›å»ºå˜æ¢
        test_transform = MedicalImageAugmentation.get_test_transform(self.config.data.input_size)
        
        # åˆ›å»ºæ•°æ®é›†
        test_dataset = Dataset(
            data_dir=self.config.data.test_path,
            transform=test_transform,
            cache_images=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = CustomDataLoader.create_dataloader(
            dataset=test_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=False,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory,
            use_weighted_sampler=False
        )
        
        class_names = test_dataset.classes
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"   ç±»åˆ«æ•°: {len(class_names)}")
        print(f"   æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        return test_loader, class_names
    
    def evaluate_model(self, model: nn.Module, test_loader: DataLoader, class_names: List[str]) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        print("ğŸ§ª å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        model.eval()
        all_predictions = []
        all_targets = []
        all_probabilities = []
        
        with torch.no_grad():
            for inputs, targets in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                outputs = model(inputs)
                probabilities = torch.softmax(outputs, dim=1)
                _, predictions = torch.max(outputs, 1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_targets, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(
            all_targets, all_predictions, 
            target_names=class_names, 
            output_dict=True
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_targets, all_predictions)
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'classification_report': class_report,
            'confusion_matrix': cm,
            'predictions': all_predictions,
            'targets': all_targets,
            'probabilities': np.array(all_probabilities),
            'class_names': class_names
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆ:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"   å¬å›ç‡: {recall:.4f}")
        print(f"   F1åˆ†æ•°: {f1:.4f}")
        
        return results
    
    def save_results(self, results: Dict, model_name: str = "model"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        # ä¿å­˜æ•°å€¼ç»“æœ
        numeric_results = {
            'accuracy': float(results['accuracy']),
            'precision': float(results['precision']),
            'recall': float(results['recall']),
            'f1_score': float(results['f1_score']),
            'classification_report': results['classification_report']
        }
        
        with open(self.save_dir / f"{model_name}_results.json", 'w', encoding='utf-8') as f:
            json.dump(numeric_results, f, indent=2, ensure_ascii=False)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(results['confusion_matrix'], results['class_names'], model_name)
        
        # ç»˜åˆ¶åˆ†ç±»æŠ¥å‘Š
        self.plot_classification_report(results['classification_report'], model_name)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.save_dir}")
    
    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str], model_name: str):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(12, 10))
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        
        plt.title(f'Confusion Matrix - {model_name}')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        plt.savefig(self.save_dir / f"{model_name}_confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_classification_report(self, report: Dict, model_name: str):
        """ç»˜åˆ¶åˆ†ç±»æŠ¥å‘Š"""
        # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        classes = [k for k in report.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]
        metrics = ['precision', 'recall', 'f1-score']
        
        data = []
        for cls in classes:
            for metric in metrics:
                data.append([cls, metric, report[cls][metric]])
        
        # åˆ›å»ºDataFrameç”¨äºç»˜å›¾
        import pandas as pd
        df = pd.DataFrame(data, columns=['Class', 'Metric', 'Score'])
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        plt.figure(figsize=(15, 8))
        
        # æŒ‰ç±»åˆ«åˆ†ç»„ç»˜åˆ¶
        x_pos = np.arange(len(classes))
        width = 0.25
        
        precision_scores = [report[cls]['precision'] for cls in classes]
        recall_scores = [report[cls]['recall'] for cls in classes]
        f1_scores = [report[cls]['f1-score'] for cls in classes]
        
        plt.bar(x_pos - width, precision_scores, width, label='Precision', alpha=0.8)
        plt.bar(x_pos, recall_scores, width, label='Recall', alpha=0.8)
        plt.bar(x_pos + width, f1_scores, width, label='F1-Score', alpha=0.8)
        
        plt.xlabel('Classes')
        plt.ylabel('Score')
        plt.title(f'Classification Report - {model_name}')
        plt.xticks(x_pos, classes, rotation=45, ha='right')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(self.save_dir / f"{model_name}_classification_report.png", dpi=300, bbox_inches='tight')
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ”¯æ°”ç®¡åˆ†ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model-path", type=str, required=True, help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default="efficientnet_b0", help="æ¨¡å‹æ¶æ„")
    parser.add_argument("--test-data-path", type=str, help="æ•°æ®è·¯å¾„")
    parser.add_argument("--batch-size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--save-dir", type=str, default="./evaluation_results", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡ (cuda/cpu/auto)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.config:
        config = Config.load_config(args.config)
    else:
        config = Config()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.model:
        config.model.model_name = args.model
    if args.test_data_path:
        config.data.test_path = args.test_data_path
    if args.batch_size:
        config.data.batch_size = args.batch_size
    if args.device != "auto":
        config.system.device = args.device
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config.data.test_path):
        print(f"âŒ æ•°æ®ä¸å­˜åœ¨: {config.data.test_path}")
        return
    
    print(f"ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ¨¡å‹æ¶æ„: {config.model.model_name}")
    print(f"æ•°æ®è·¯å¾„: {config.data.test_path}")
    print(f"è®¾å¤‡: {config.system.device}")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleEvaluator(config, args.save_dir)
    
    try:
        # åŠ è½½æ¨¡å‹
        model = evaluator.load_model(args.model_path)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader, class_names = evaluator.create_test_loader()
        
        # è¯„ä¼°æ¨¡å‹
        results = evaluator.evaluate_model(model, test_loader, class_names)
        
        # ä¿å­˜ç»“æœ
        model_name = Path(args.model_path).stem
        evaluator.save_results(results, model_name)
        
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.save_dir}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()