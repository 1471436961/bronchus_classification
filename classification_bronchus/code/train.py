#!/usr/bin/env python3
"""
é«˜æ•ˆé¢„å¤„ç†çš„è®­ç»ƒè„šæœ¬
æ•´åˆäº†æ‰€æœ‰æ•°æ®é¢„å¤„ç†åŠŸèƒ½
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils import (
    setup_training_logger, get_logger, init_default_logger,
    get_device, move_to_device, clear_cuda_cache,
    validate_config, ValidationError,
    handle_exceptions, profile_function,
    MemoryManager, global_monitor
)

init_default_logger()
logger = get_logger(__name__)
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from config import Config, ConfigTemplates
from data_process import (
    DatasetSplitter, 
    MedicalImageAugmentation, 
    Dataset, 
    CustomDataLoader,
    calculate_dataset_statistics
)


class OptimizedTrainer:
    """é«˜æ•ˆçš„è®­ç»ƒå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.device = torch.device(config.system.device)
        
        # è®¾ç½®éšæœºç§å­
        self._set_seed(config.system.seed)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.system.output_dir) / f"train_{time.strftime('%Y%m%d_%H%M%S')}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config.save_config(str(self.output_dir / "config.json"))
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.writer = SummaryWriter(str(self.output_dir / "tensorboard"))
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'lr': []
        }
        
        # æœ€ä½³æŒ‡æ ‡
        self.best_val_acc = 0.0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        print(f"ğŸš€ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        
        if self.config.system.deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    def prepare_data(self) -> Tuple[torch.utils.data.DataLoader, ...]:
        """å‡†å¤‡æ•°æ®"""
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config.data.train_path):
            print("âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨åˆ’åˆ†æ•°æ®é›†...")
            self._split_dataset()
        
        # è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config.data.use_custom_normalization:
            print("ğŸ“ˆ è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
            stats = calculate_dataset_statistics(self.config.data.train_path)
            # æ›´æ–°æ ‡å‡†åŒ–å‚æ•°
            # è¿™é‡Œå¯ä»¥åŠ¨æ€æ›´æ–°å¢å¼ºç­–ç•¥ä¸­çš„æ ‡å‡†åŒ–å‚æ•°
        
        # åˆ›å»ºæ•°æ®å¢å¼ºç­–ç•¥
        input_size = self.config.data.input_size
        
        if self.config.data.augmentation_level == "light":
            train_transform = MedicalImageAugmentation.get_light_augmentation(input_size)
        elif self.config.data.augmentation_level == "heavy":
            train_transform = MedicalImageAugmentation.get_heavy_augmentation(input_size)
        else:  # medium
            train_transform = MedicalImageAugmentation.get_medium_augmentation(input_size)
        
        val_transform = MedicalImageAugmentation.get_test_transform(input_size)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = Dataset(
            data_dir=self.config.data.train_path,
            transform=train_transform,
            cache_images=self.config.data.cache_images
        )
        
        val_dataset = Dataset(
            data_dir=self.config.data.val_path,
            transform=val_transform,
            class_to_idx=train_dataset.class_to_idx,  # ä¿æŒä¸€è‡´çš„ç±»åˆ«æ˜ å°„
            cache_images=self.config.data.cache_images
        )
        
        # ä¿å­˜ç±»åˆ«æ˜ å°„
        with open(self.output_dir / "class_to_idx.json", 'w') as f:
            json.dump(train_dataset.class_to_idx, f, indent=2)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = CustomDataLoader.create_dataloader(
            dataset=train_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=True,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory,
            persistent_workers=self.config.data.persistent_workers,
            prefetch_factor=self.config.data.prefetch_factor,
            use_weighted_sampler=self.config.data.use_weighted_sampler
        )
        
        val_loader = CustomDataLoader.create_dataloader(
            dataset=val_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=False,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory,
            persistent_workers=self.config.data.persistent_workers,
            prefetch_factor=self.config.data.prefetch_factor,
            use_weighted_sampler=False
        )
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"   ç±»åˆ«æ•°: {len(train_dataset.classes)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.config.data.batch_size}")
        print(f"   æ•°æ®å¢å¼º: {self.config.data.augmentation_level}")
        
        return train_loader, val_loader, train_dataset.classes
    
    def _split_dataset(self):
        """è‡ªåŠ¨åˆ’åˆ†æ•°æ®é›†"""
        # å‡è®¾åŸå§‹æ•°æ®åœ¨ data_root ç›®å½•ä¸‹
        org_path = self.config.data.data_root.replace("/data_split", "")
        if os.path.exists(org_path):
            splitter = DatasetSplitter(random_state=self.config.system.seed)
            splitter.split_data_stratified(
                org_path=org_path,
                split_path=self.config.data.data_root,
                train_ratio=self.config.data.train_ratio,
                val_ratio=self.config.data.val_ratio,
                test_ratio=self.config.data.test_ratio,
                min_samples_per_class=self.config.data.min_samples_per_class
            )
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŸå§‹æ•°æ®ç›®å½•: {org_path}")
    
    def create_model(self) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        print(f"ğŸ—ï¸ åˆ›å»ºæ¨¡å‹: {self.config.model.model_name}")
        
        model = self.config.model.create_model()
        model = model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   è¾“å…¥å°ºå¯¸: {self.config.data.input_size}")
        
        return model
    
    def create_optimizer_and_scheduler(self, model: nn.Module) -> Tuple[optim.Optimizer, optim.lr_scheduler._LRScheduler]:
        """åˆ›å»ºè®­ç»ƒå™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # åˆ›å»ºè®­ç»ƒå™¨
        if self.config.training.optimizer.lower() == "adam":
            optimizer = optim.Adam(
                model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == "adamw":
            optimizer = optim.AdamW(
                model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == "sgd":
            optimizer = optim.SGD(
                model.parameters(),
                lr=self.config.training.learning_rate,
                momentum=self.config.training.momentum,
                weight_decay=self.config.training.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒå™¨: {self.config.training.optimizer}")
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.training.scheduler.lower() == "cosine":
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=self.config.training.epochs
            )
        elif self.config.training.scheduler.lower() == "step":
            scheduler = optim.lr_scheduler.StepLR(
                optimizer, 
                step_size=self.config.training.step_size,
                gamma=self.config.training.gamma
            )
        elif self.config.training.scheduler.lower() == "multistep":
            scheduler = optim.lr_scheduler.MultiStepLR(
                optimizer,
                milestones=self.config.training.milestones,
                gamma=self.config.training.gamma
            )
        elif self.config.training.scheduler.lower() == "plateau":
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                patience=self.config.training.patience,
                factor=self.config.training.gamma
            )
        else:
            scheduler = None
        
        print(f"âœ… è®­ç»ƒå™¨: {self.config.training.optimizer}")
        print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.config.training.scheduler}")
        
        return optimizer, scheduler
    
    def create_loss_function(self, class_weights: Optional[torch.Tensor] = None) -> nn.Module:
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        if self.config.training.loss_function == "cross_entropy":
            if self.config.training.label_smoothing > 0:
                criterion = nn.CrossEntropyLoss(
                    weight=class_weights,
                    label_smoothing=self.config.training.label_smoothing
                )
            else:
                criterion = nn.CrossEntropyLoss(weight=class_weights)
        else:
            # å¯ä»¥æ·»åŠ å…¶ä»–æŸå¤±å‡½æ•°
            criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        return criterion.to(self.device)
    
    def train_epoch(
        self, 
        model: nn.Module, 
        train_loader: torch.utils.data.DataLoader,
        optimizer: optim.Optimizer,
        criterion: nn.Module,
        scaler: Optional[torch.cuda.amp.GradScaler] = None,
        epoch: int = 0
    ) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.config.training.epochs}")
        
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            if self.config.training.use_amp and scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % self.config.training.accumulate_grad_batches == 0:
                    if self.config.training.gradient_clip_val > 0:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.training.gradient_clip_val)
                    
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                loss.backward()
                
                if (batch_idx + 1) % self.config.training.accumulate_grad_batches == 0:
                    if self.config.training.gradient_clip_val > 0:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.training.gradient_clip_val)
                    
                    optimizer.step()
                    optimizer.zero_grad()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            if batch_idx % self.config.system.log_every_n_steps == 0:
                current_lr = optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'LR': f'{current_lr:.2e}'
                })
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate_epoch(
        self, 
        model: nn.Module, 
        val_loader: torch.utils.data.DataLoader,
        criterion: nn.Module
    ) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in tqdm(val_loader, desc="Validating"):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def save_checkpoint(self, model: nn.Module, optimizer: optim.Optimizer, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_acc': self.best_val_acc,
            'config': self.config.__dict__
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.output_dir / "latest_checkpoint.pth")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, self.output_dir / "best_model.pth")
            torch.save(model.state_dict(), self.output_dir / f"{self.config.model.model_name}_best.pth")
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.history['train_loss']) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, self.history['train_loss'], 'b-', label='Train Loss')
        ax1.plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, self.history['train_acc'], 'b-', label='Train Acc')
        ax2.plot(epochs, self.history['val_acc'], 'r-', label='Val Acc')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(epochs, self.history['lr'], 'g-', label='Learning Rate')
        ax3.set_title('Learning Rate Schedule')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.set_yscale('log')
        ax3.legend()
        ax3.grid(True)
        
        # éªŒè¯å‡†ç¡®ç‡è¯¦ç»†è§†å›¾
        ax4.plot(epochs, self.history['val_acc'], 'r-', linewidth=2)
        ax4.axhline(y=max(self.history['val_acc']), color='g', linestyle='--', 
                   label=f'Best: {max(self.history["val_acc"]):.2f}%')
        ax4.set_title('Validation Accuracy (Detailed)')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Accuracy (%)')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "training_history.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®
        train_loader, val_loader, classes = self.prepare_data()
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # åˆ›å»ºè®­ç»ƒå™¨å’Œè°ƒåº¦å™¨
        optimizer, scheduler = self.create_optimizer_and_scheduler(model)
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        class_weights = None
        if self.config.data.use_class_weights:
            # è¿™é‡Œå¯ä»¥ä»è®­ç»ƒæ•°æ®é›†è®¡ç®—ç±»åˆ«æƒé‡
            pass
        criterion = self.create_loss_function(class_weights)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        scaler = torch.cuda.amp.GradScaler() if self.config.training.use_amp else None
        
        # è®­ç»ƒå¾ªç¯
        start_time = time.time()
        
        for epoch in range(self.config.training.epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(
                model, train_loader, optimizer, criterion, scaler, epoch
            )
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(model, val_loader, criterion)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            if scheduler is not None:
                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(val_loss)
                else:
                    scheduler.step()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['lr'].append(current_lr)
            
            # TensorBoardæ—¥å¿—
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar('Loss/Val', val_loss, epoch)
            self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
            self.writer.add_scalar('Accuracy/Val', val_acc, epoch)
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(model, optimizer, epoch, is_best)
            
            # æ‰“å°epochç»“æœ
            print(f"Epoch {epoch+1}/{self.config.training.epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"  LR: {current_lr:.2e}, Best Val Acc: {self.best_val_acc:.2f}%")
            
            # æ—©åœæ£€æŸ¥
            if (self.config.training.early_stopping and 
                self.patience_counter >= self.config.training.early_stopping_patience):
                print(f"æ—©åœè§¦å‘ï¼è¿ç»­ {self.patience_counter} ä¸ªepochæ— æ”¹å–„")
                break
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        self.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(self.output_dir / "training_history.json", 'w') as f:
            json.dump(self.history, f, indent=2)
        
        self.writer.close()
        
        return model


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é«˜æ•ˆé¢„å¤„ç†è®­ç»ƒè„šæœ¬")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--template", type=str, choices=["fast", "high_quality", "small_dataset", "large_model"],
                       help="ä½¿ç”¨é¢„å®šä¹‰é…ç½®æ¨¡æ¿")
    parser.add_argument("--model", type=str, default="efficientnet_b0", help="æ¨¡å‹åç§°")
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--augmentation", type=str, choices=["light", "medium", "heavy"], 
                       default="medium", help="æ•°æ®å¢å¼ºå¼ºåº¦")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.config:
        config = Config.load_config(args.config)
    elif args.template:
        if args.template == "fast":
            config = ConfigTemplates.get_fast_training_config()
        elif args.template == "high_quality":
            config = ConfigTemplates.get_high_quality_config()
        elif args.template == "small_dataset":
            config = ConfigTemplates.get_small_dataset_config()
        elif args.template == "large_model":
            config = ConfigTemplates.get_large_model_config()
    else:
        config = Config()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.model:
        config.model.model_name = args.model
    if args.epochs:
        config.training.epochs = args.epochs
    if args.batch_size:
        config.data.batch_size = args.batch_size
    if args.lr:
        config.training.learning_rate = args.lr
    if args.augmentation:
        config.data.augmentation_level = args.augmentation
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = OptimizedTrainer(config)
    model = trainer.train()
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()