#!/usr/bin/env python3
"""
é«˜çº§è¯„ä¼°è„šæœ¬ - æ”¯æ°”ç®¡åˆ†ç±»é¡¹ç›®
æä¾›å…¨é¢çš„æ¨¡å‹è¯„ä¼°åŠŸèƒ½å’Œæ·±åº¦åˆ†æ
"""

import os
import sys
import argparse
import json
import shutil
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Tuple, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    balanced_accuracy_score, precision_recall_fscore_support,
    precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, auc
)
from loguru import logger
import pandas as pd

from config import Config, ConfigTemplates
from data_process import Dataset, CustomDataLoader, MedicalImageAugmentation

# é˜²æ­¢OMPé”™è¯¯
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class AdvancedEvaluator:
    """é«˜çº§æ¨¡å‹è¯„ä¼°å™¨ - æä¾›å…¨é¢çš„æ¨¡å‹è¯„ä¼°å’Œåˆ†æåŠŸèƒ½"""
    
    def __init__(self, config: Config, save_dir: str = "./evaluation_results"):
        self.config = config
        self.device = torch.device(config.system.device)
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.images_dir = self.save_dir / "error_images"
        self.plots_dir = self.save_dir / "plots"
        self.reports_dir = self.save_dir / "reports"
        
        for dir_path in [self.images_dir, self.plots_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        cur_time = datetime.datetime.now().strftime('test_%Y%m%d_%H%M%S')
        log_path = self.save_dir / f"{cur_time}.log"
        logger.add(str(log_path), rotation="10 MB", retention="7 days")
        logger.info(f"Advanced evaluator initialized at {cur_time}")
        
    def load_model(self, model_path: str) -> nn.Module:
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"Loading model from: {model_path}")
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.config.model.create_model()
        
        # åŠ è½½æƒé‡
        if model_path.endswith('.pth'):
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                    if 'best_acc' in checkpoint:
                        logger.info(f"Model best accuracy: {checkpoint['best_acc']}")
                elif 'best_weight' in checkpoint:
                    # æ—§ç‰ˆæœ¬æ ¼å¼
                    weights = checkpoint['best_weight']
                    model.load_state_dict(weights)
                    if 'best_acc' in checkpoint:
                        logger.info(f"Model best accuracy: {checkpoint['best_acc']}")
                else:
                    model.load_state_dict(checkpoint)
            else:
                model.load_state_dict(checkpoint)
        
        model = model.to(self.device)
        model.eval()
        
        logger.info("Model loaded successfully")
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return model
    
    def create_test_loader(self) -> Tuple[DataLoader, List[str], List[str]]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        logger.info("Creating data loader...")
        print(f"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # åˆ›å»ºå˜æ¢
        test_transform = MedicalImageAugmentation.get_test_transform(self.config.data.input_size)
        
        # åˆ›å»ºæ•°æ®é›†
        test_dataset = Dataset(
            data_dir=self.config.data.test_path,
            transform=test_transform,
            cache_images=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = CustomDataLoader.create_dataloader(
            dataset=test_dataset,
            batch_size=self.config.data.batch_size,
            shuffle=False,
            num_workers=self.config.data.num_workers,
            pin_memory=self.config.data.pin_memory,
            use_weighted_sampler=False
        )
        
        class_names = test_dataset.classes
        
        # è·å–å›¾åƒè·¯å¾„ï¼ˆç”¨äºé”™è¯¯åˆ†æï¼‰
        image_paths = []
        if hasattr(test_dataset, 'samples'):
            image_paths = [sample[0] for sample in test_dataset.samples]
        elif hasattr(test_dataset, 'imgs'):
            image_paths = [img[0] for img in test_dataset.imgs]
        
        logger.info(f"Test dataset created: {len(test_dataset)} samples, {len(class_names)} classes")
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"   ç±»åˆ«æ•°: {len(class_names)}")
        print(f"   æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        return test_loader, class_names, image_paths
    
    def evaluate_model(self, model: nn.Module, test_loader: DataLoader, 
                      class_names: List[str], image_paths: List[str]) -> Dict:
        """å…¨é¢è¯„ä¼°æ¨¡å‹"""
        logger.info("Starting comprehensive model evaluation...")
        print("ğŸ§ª å¼€å§‹å…¨é¢æ¨¡å‹è¯„ä¼°...")
        
        model.eval()
        all_predictions = []
        all_targets = []
        all_probabilities = []
        
        with torch.no_grad():
            for inputs, targets in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                outputs = model(inputs)
                probabilities = torch.softmax(outputs, dim=1)
                _, predictions = torch.max(outputs, 1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        all_probabilities = np.array(all_probabilities)
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(all_targets, all_predictions)
        balanced_acc = balanced_accuracy_score(all_targets, all_predictions)
        
        # è®¡ç®—macroå’Œweightedå¹³å‡æŒ‡æ ‡
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            all_targets, all_predictions, average='macro'
        )
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            all_targets, all_predictions, average='weighted'
        )
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(
            all_targets, all_predictions, average=None
        )
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(
            all_targets, all_predictions, 
            target_names=class_names, 
            output_dict=True
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_targets, all_predictions)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ï¼ˆæ—§ç‰ˆæœ¬çš„æ–¹æ³•ï¼‰
        accuracy_per_class = []
        for i in range(len(class_names)):
            if i < len(cm):
                TP = cm[i, i]
                total = np.sum(cm[i, :])
                class_accuracy = TP / total if total != 0 else 0
                accuracy_per_class.append(class_accuracy)
            else:
                accuracy_per_class.append(0.0)
        
        # è®¡ç®—ROC AUCï¼ˆå¦‚æœæ˜¯äºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ï¼‰
        roc_auc_scores = {}
        if len(class_names) == 2:
            # äºŒåˆ†ç±»
            roc_auc_scores['binary'] = roc_auc_score(all_targets, all_probabilities[:, 1])
        elif len(class_names) > 2:
            # å¤šåˆ†ç±» - è®¡ç®—æ¯ä¸ªç±»åˆ«çš„OvR AUC
            try:
                roc_auc_scores['macro'] = roc_auc_score(all_targets, all_probabilities, 
                                                      multi_class='ovr', average='macro')
                roc_auc_scores['weighted'] = roc_auc_score(all_targets, all_probabilities, 
                                                         multi_class='ovr', average='weighted')
            except ValueError as e:
                logger.warning(f"Could not compute ROC AUC: {e}")
                roc_auc_scores = {}
        
        # æ‰¾å‡ºé¢„æµ‹é”™è¯¯çš„æ ·æœ¬
        error_indices = np.where(all_predictions != all_targets)[0]
        error_samples = []
        for idx in error_indices:
            if idx < len(image_paths):
                error_samples.append({
                    'image_path': image_paths[idx],
                    'true_label': int(all_targets[idx]),
                    'predicted_label': int(all_predictions[idx]),
                    'true_class': class_names[all_targets[idx]],
                    'predicted_class': class_names[all_predictions[idx]],
                    'confidence': float(all_probabilities[idx][all_predictions[idx]])
                })
        
        results = {
            # åŸºç¡€æŒ‡æ ‡
            'accuracy': float(accuracy),
            'balanced_accuracy': float(balanced_acc),
            
            # Macroå¹³å‡æŒ‡æ ‡
            'precision_macro': float(precision_macro),
            'recall_macro': float(recall_macro),
            'f1_score_macro': float(f1_macro),
            
            # Weightedå¹³å‡æŒ‡æ ‡
            'precision_weighted': float(precision_weighted),
            'recall_weighted': float(recall_weighted),
            'f1_score_weighted': float(f1_weighted),
            
            # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
            'precision_per_class': precision_per_class.tolist(),
            'recall_per_class': recall_per_class.tolist(),
            'f1_score_per_class': f1_per_class.tolist(),
            'accuracy_per_class': accuracy_per_class,
            'support_per_class': support_per_class.tolist(),
            
            # ROC AUC
            'roc_auc_scores': roc_auc_scores,
            
            # è¯¦ç»†æ•°æ®
            'classification_report': class_report,
            'confusion_matrix': cm.tolist(),
            'predictions': all_predictions.tolist(),
            'targets': all_targets.tolist(),
            'probabilities': all_probabilities.tolist(),
            'class_names': class_names,
            'error_samples': error_samples,
            
            # ç»Ÿè®¡ä¿¡æ¯
            'total_samples': len(all_targets),
            'error_count': len(error_samples),
            'error_rate': len(error_samples) / len(all_targets)
        }
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"âœ… è¯„ä¼°å®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(all_targets)}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
        print(f"   ç²¾ç¡®ç‡(macro): {precision_macro:.4f}")
        print(f"   å¬å›ç‡(macro): {recall_macro:.4f}")
        print(f"   F1åˆ†æ•°(macro): {f1_macro:.4f}")
        print(f"   é”™è¯¯æ ·æœ¬æ•°: {len(error_samples)}")
        print(f"   é”™è¯¯ç‡: {len(error_samples) / len(all_targets):.4f}")
        
        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print(f"\nğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i, (class_name, acc) in enumerate(zip(class_names, accuracy_per_class)):
            print(f"   {class_name}: {acc:.4f}")
        
        logger.info(f"Evaluation completed: accuracy={accuracy:.4f}, balanced_accuracy={balanced_acc:.4f}")
        
        return results
    
    def save_error_images(self, error_samples: List[Dict], max_errors: int = 100):
        """ä¿å­˜é¢„æµ‹é”™è¯¯çš„å›¾åƒ"""
        if not error_samples:
            print("âœ… æ²¡æœ‰é¢„æµ‹é”™è¯¯çš„æ ·æœ¬")
            return
        
        print(f"ğŸ’¾ ä¿å­˜é¢„æµ‹é”™è¯¯çš„å›¾åƒ (æœ€å¤š{max_errors}å¼ )...")
        logger.info(f"Saving error images: {len(error_samples)} errors found")
        
        saved_count = 0
        for error in error_samples[:max_errors]:
            try:
                src_path = error['image_path']
                if os.path.exists(src_path):
                    # åˆ›å»ºæ–°çš„æ–‡ä»¶å
                    base_name = os.path.basename(src_path)
                    name, ext = os.path.splitext(base_name)
                    new_name = f"{name}_true{error['true_label']}_pred{error['predicted_label']}_conf{error['confidence']:.3f}{ext}"
                    
                    dst_path = self.images_dir / new_name
                    shutil.copy2(src_path, dst_path)
                    
                    # è®°å½•æ—¥å¿—
                    logger.info(f"Error image saved: {src_path} -> true:{error['true_class']}, pred:{error['predicted_class']}, conf:{error['confidence']:.3f}")
                    saved_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to save error image {error['image_path']}: {e}")
        
        print(f"âœ… å·²ä¿å­˜ {saved_count} å¼ é”™è¯¯é¢„æµ‹å›¾åƒåˆ°: {self.images_dir}")
    
    def save_results(self, results: Dict, model_name: str = "model"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        logger.info("Saving evaluation results...")
        
        # ä¿å­˜æ•°å€¼ç»“æœ
        numeric_results = {
            'model_name': model_name,
            'evaluation_time': datetime.datetime.now().isoformat(),
            'total_samples': results['total_samples'],
            'error_count': results['error_count'],
            'error_rate': results['error_rate'],
            
            # åŸºç¡€æŒ‡æ ‡
            'accuracy': results['accuracy'],
            'balanced_accuracy': results['balanced_accuracy'],
            
            # Macroå¹³å‡æŒ‡æ ‡
            'precision_macro': results['precision_macro'],
            'recall_macro': results['recall_macro'],
            'f1_score_macro': results['f1_score_macro'],
            
            # Weightedå¹³å‡æŒ‡æ ‡
            'precision_weighted': results['precision_weighted'],
            'recall_weighted': results['recall_weighted'],
            'f1_score_weighted': results['f1_score_weighted'],
            
            # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
            'per_class_metrics': {
                'class_names': results['class_names'],
                'precision': results['precision_per_class'],
                'recall': results['recall_per_class'],
                'f1_score': results['f1_score_per_class'],
                'accuracy': results['accuracy_per_class'],
                'support': results['support_per_class']
            },
            
            # ROC AUC
            'roc_auc_scores': results['roc_auc_scores'],
            
            # åˆ†ç±»æŠ¥å‘Š
            'classification_report': results['classification_report']
        }
        
        # ä¿å­˜JSONç»“æœ
        with open(self.reports_dir / f"{model_name}_results.json", 'w', encoding='utf-8') as f:
            json.dump(numeric_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†çš„é”™è¯¯åˆ†æ
        if results['error_samples']:
            with open(self.reports_dir / f"{model_name}_error_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(results['error_samples'], f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜é¢„æµ‹é”™è¯¯çš„å›¾åƒ
        self.save_error_images(results['error_samples'])
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.plot_confusion_matrix(np.array(results['confusion_matrix']), results['class_names'], model_name)
        self.plot_classification_report(results['classification_report'], model_name)
        self.plot_per_class_metrics(results, model_name)
        self.plot_error_analysis(results, model_name)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.save_dir}")
        logger.info(f"Results saved to: {self.save_dir}")
    
    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str], model_name: str):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(12, 10))
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        
        plt.title(f'Confusion Matrix (%) - {model_name}')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        plt.savefig(self.plots_dir / f"{model_name}_confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # åŒæ—¶ä¿å­˜æ•°å€¼ç‰ˆæœ¬
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        
        plt.title(f'Confusion Matrix (Count) - {model_name}')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        plt.savefig(self.plots_dir / f"{model_name}_confusion_matrix_count.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_classification_report(self, report: Dict, model_name: str):
        """ç»˜åˆ¶åˆ†ç±»æŠ¥å‘Š"""
        classes = [k for k in report.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]
        metrics = ['precision', 'recall', 'f1-score']
        
        # åˆ›å»ºDataFrame
        data = []
        for cls in classes:
            for metric in metrics:
                data.append([cls, metric, report[cls][metric]])
        
        df = pd.DataFrame(data, columns=['Class', 'Metric', 'Score'])
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        plt.figure(figsize=(15, 8))
        
        x_pos = np.arange(len(classes))
        width = 0.25
        
        precision_scores = [report[cls]['precision'] for cls in classes]
        recall_scores = [report[cls]['recall'] for cls in classes]
        f1_scores = [report[cls]['f1-score'] for cls in classes]
        
        plt.bar(x_pos - width, precision_scores, width, label='Precision', alpha=0.8)
        plt.bar(x_pos, recall_scores, width, label='Recall', alpha=0.8)
        plt.bar(x_pos + width, f1_scores, width, label='F1-Score', alpha=0.8)
        
        plt.xlabel('Classes')
        plt.ylabel('Score')
        plt.title(f'Classification Report - {model_name}')
        plt.xticks(x_pos, classes, rotation=45, ha='right')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(self.plots_dir / f"{model_name}_classification_report.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_per_class_metrics(self, results: Dict, model_name: str):
        """ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡"""
        class_names = results['class_names']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Per-Class Metrics - {model_name}', fontsize=16)
        
        # ç²¾ç¡®ç‡
        axes[0, 0].bar(class_names, results['precision_per_class'])
        axes[0, 0].set_title('Precision per Class')
        axes[0, 0].set_ylabel('Precision')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # å¬å›ç‡
        axes[0, 1].bar(class_names, results['recall_per_class'])
        axes[0, 1].set_title('Recall per Class')
        axes[0, 1].set_ylabel('Recall')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # F1åˆ†æ•°
        axes[1, 0].bar(class_names, results['f1_score_per_class'])
        axes[1, 0].set_title('F1-Score per Class')
        axes[1, 0].set_ylabel('F1-Score')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # å‡†ç¡®ç‡
        axes[1, 1].bar(class_names, results['accuracy_per_class'])
        axes[1, 1].set_title('Accuracy per Class')
        axes[1, 1].set_ylabel('Accuracy')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / f"{model_name}_per_class_metrics.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_error_analysis(self, results: Dict, model_name: str):
        """ç»˜åˆ¶é”™è¯¯åˆ†æå›¾"""
        if not results['error_samples']:
            return
        
        # é”™è¯¯åˆ†å¸ƒåˆ†æ
        error_by_true_class = {}
        error_by_pred_class = {}
        
        for error in results['error_samples']:
            true_class = error['true_class']
            pred_class = error['predicted_class']
            
            error_by_true_class[true_class] = error_by_true_class.get(true_class, 0) + 1
            error_by_pred_class[pred_class] = error_by_pred_class.get(pred_class, 0) + 1
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(f'Error Analysis - {model_name}', fontsize=16)
        
        # æŒ‰çœŸå®ç±»åˆ«çš„é”™è¯¯åˆ†å¸ƒ
        if error_by_true_class:
            axes[0].bar(error_by_true_class.keys(), error_by_true_class.values())
            axes[0].set_title('Errors by True Class')
            axes[0].set_ylabel('Error Count')
            axes[0].tick_params(axis='x', rotation=45)
        
        # æŒ‰é¢„æµ‹ç±»åˆ«çš„é”™è¯¯åˆ†å¸ƒ
        if error_by_pred_class:
            axes[1].bar(error_by_pred_class.keys(), error_by_pred_class.values())
            axes[1].set_title('Errors by Predicted Class')
            axes[1].set_ylabel('Error Count')
            axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / f"{model_name}_error_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é«˜çº§æ”¯æ°”ç®¡åˆ†ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model-path", type=str, required=True, help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--template", type=str, help="é…ç½®æ¨¡æ¿")
    parser.add_argument("--model", type=str, default="efficientnet_b0", help="æ¨¡å‹æ¶æ„")
    parser.add_argument("--test-data-path", type=str, help="æ•°æ®è·¯å¾„")
    parser.add_argument("--batch-size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--save-dir", type=str, default="./evaluation_results_enhanced", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡ (cuda/cpu/auto)")
    parser.add_argument("--max-error-images", type=int, default=100, help="æœ€å¤§ä¿å­˜é”™è¯¯å›¾åƒæ•°é‡")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    if args.config:
        config = Config.load_config(args.config)
    elif args.template:
        config = ConfigTemplates.get_template(args.template)
    else:
        config = Config()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.model:
        config.model.model_name = args.model
    if args.test_data_path:
        config.data.test_path = args.test_data_path
    if args.batch_size:
        config.data.batch_size = args.batch_size
    if args.device != "auto":
        config.system.device = args.device
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config.data.test_path):
        print(f"âŒ æ•°æ®ä¸å­˜åœ¨: {config.data.test_path}")
        return
    
    print(f"ğŸš€ å¼€å§‹é«˜çº§æ¨¡å‹è¯„ä¼°")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ¨¡å‹æ¶æ„: {config.model.model_name}")
    print(f"æ•°æ®è·¯å¾„: {config.data.test_path}")
    print(f"è®¾å¤‡: {config.system.device}")
    print(f"ç»“æœä¿å­˜: {args.save_dir}")
    
    # åˆ›å»ºé«˜çº§è¯„ä¼°å™¨
    evaluator = AdvancedEvaluator(config, args.save_dir)
    
    try:
        # åŠ è½½æ¨¡å‹
        model = evaluator.load_model(args.model_path)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader, class_names, image_paths = evaluator.create_test_loader()
        
        # è¯„ä¼°æ¨¡å‹
        results = evaluator.evaluate_model(model, test_loader, class_names, image_paths)
        
        # ä¿å­˜ç»“æœ
        model_name = Path(args.model_path).stem
        evaluator.save_results(results, model_name)
        
        print(f"\nğŸ‰ é«˜çº§è¯„ä¼°å®Œæˆï¼")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.save_dir}")
        print(f"  - æŠ¥å‘Š: {args.save_dir}/reports/")
        print(f"  - å›¾è¡¨: {args.save_dir}/plots/")
        print(f"  - é”™è¯¯å›¾åƒ: {args.save_dir}/error_images/")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.error(f"Evaluation failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()